{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "turned-electric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point  crazy   available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar    joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor    u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don t think he goes to usf  he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                            Message\n",
       "0      0  go until jurong point  crazy   available only ...\n",
       "1      0                      ok lar    joking wif u oni   \n",
       "2      1  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3      0  u dun say so early hor    u c already then say   \n",
       "4      0  nah i don t think he goes to usf  he lives aro..."
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dframe = pd.read_csv(\"../message.csv\")\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-explorer",
   "metadata": {},
   "source": [
    "**Q-1.1 From the training set, compute the total number of unique words in the set and the count\n",
    "of each unique word in each message. Hence, if there are N unique words and M messages\n",
    "in the training set, then the count of each unique word for all messages should result in a M × N matrix. You may want to use DataFrame and dictionary objects to accomplish this.\n",
    "You may also use split() to ignore whitespace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "economic-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8753\n"
     ]
    }
   ],
   "source": [
    "# Finds all the uniq words.\n",
    "uniq_words = set(w for message in dframe[\"Message\"] for w in message.split(\" \") if w != '')\n",
    "print(len(uniq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "tired-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-145-e16fcc3d0720>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resulting_frame[word][index_c] = resulting_frame[word][index_c] + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>secs</th>\n",
       "      <th>k52</th>\n",
       "      <th>sitter</th>\n",
       "      <th>uploaded</th>\n",
       "      <th>kotees</th>\n",
       "      <th>hottest</th>\n",
       "      <th>crushes</th>\n",
       "      <th>camp</th>\n",
       "      <th>rise</th>\n",
       "      <th>...</th>\n",
       "      <th>increments</th>\n",
       "      <th>walking</th>\n",
       "      <th>shite</th>\n",
       "      <th>1013</th>\n",
       "      <th>westonzoyland</th>\n",
       "      <th>09058095201</th>\n",
       "      <th>prices</th>\n",
       "      <th>dedicated</th>\n",
       "      <th>strewn</th>\n",
       "      <th>lane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  secs  k52  sitter  uploaded  kotees  hottest  crushes  camp  rise  \\\n",
       "0      0   0.0  0.0     0.0       0.0     0.0      0.0      0.0   0.0   0.0   \n",
       "1      0   0.0  0.0     0.0       0.0     0.0      0.0      0.0   0.0   0.0   \n",
       "2      1   0.0  0.0     0.0       0.0     0.0      0.0      0.0   0.0   0.0   \n",
       "3      0   0.0  0.0     0.0       0.0     0.0      0.0      0.0   0.0   0.0   \n",
       "4      0   0.0  0.0     0.0       0.0     0.0      0.0      0.0   0.0   0.0   \n",
       "\n",
       "   ...  increments  walking  shite  1013  westonzoyland  09058095201  prices  \\\n",
       "0  ...         0.0      0.0    0.0   0.0            0.0          0.0     0.0   \n",
       "1  ...         0.0      0.0    0.0   0.0            0.0          0.0     0.0   \n",
       "2  ...         0.0      0.0    0.0   0.0            0.0          0.0     0.0   \n",
       "3  ...         0.0      0.0    0.0   0.0            0.0          0.0     0.0   \n",
       "4  ...         0.0      0.0    0.0   0.0            0.0          0.0     0.0   \n",
       "\n",
       "   dedicated  strewn  lane  \n",
       "0        0.0     0.0   0.0  \n",
       "1        0.0     0.0   0.0  \n",
       "2        0.0     0.0   0.0  \n",
       "3        0.0     0.0   0.0  \n",
       "4        0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 8754 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function returns the MxN matrix given the dataframe.\n",
    "# Where each cell represents the times a word appears in a sentence.\n",
    "def get_dframe_with_train_indexes(train_data):\n",
    "    resulting_frame = train_data[[\"Label\"]].copy(deep=True)\n",
    "    \n",
    "    # we can also generate uniq_words from just the training set\n",
    "    zero_arr = np.zeros((len(uniq_words),len(train_data)))\n",
    "    for i, word in enumerate(uniq_words):\n",
    "        resulting_frame[word] = zero_arr[i]\n",
    "    \n",
    "    for message, index_c in zip(train_data[\"Message\"], resulting_frame.index):\n",
    "        for word in message.split(\" \"):\n",
    "            if word != '':\n",
    "                resulting_frame[word][index_c] = resulting_frame[word][index_c] + 1\n",
    "    return resulting_frame\n",
    "\n",
    "word_count_dframe = get_dframe_with_train_indexes(dframe)\n",
    "\n",
    "print(word_count_dframe[dframe[\"Message\"][0].split(\" \")[3]][0])\n",
    "word_count_dframe.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-title",
   "metadata": {},
   "source": [
    "Note: *In above function named get_dframe_with_train_indexes, since the matrix is sparse, as in out of 8k unique words, only ~10 are present in a sentence. So to avoid big calculations, we have initialized every value with zero, and only changed the value for those words which are present in the message. It makes our program faster than having to calculate the value for every word in a message.*\n",
    "\n",
    "We see that our program outputs the value of \"1.0\" for a word that exists in the message, which confirms that our matrix has been calculated correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-submission",
   "metadata": {},
   "source": [
    "**Q 1.2 Perform maximum likelihood estimation to determine the prior and class conditional probabilities of the training set (e.g. compute P(y = 1), P(y = 0), P(xi\n",
    "|y = 0), and P(xi\n",
    "|y = 1)) ,\n",
    "where xi represents the i-th unique word. Be sure to confirm that these are indeed probabilities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fixed-procedure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam = 0.13406317300789664 and probability of not spam = 0.8659368269921034\n"
     ]
    }
   ],
   "source": [
    "# This function calculates the class probabilities, given the dataframe\n",
    "def get_class_probs(word_count_data):\n",
    "    # P(y=1) = #(1's) / total\n",
    "    # P(y=0) = #(0's) / total\n",
    "\n",
    "    count_y_1 = np.count_nonzero(word_count_data[\"Label\"] == 1)\n",
    "    count_y_0 = np.count_nonzero(word_count_data[\"Label\"] == 0)\n",
    "\n",
    "    p_y_1 = count_y_1 / len(word_count_data)    \n",
    "    p_y_0 = count_y_0 / len(word_count_data)\n",
    "    return p_y_1, p_y_0\n",
    "\n",
    "p_y_1,p_y_0 = get_class_probs(word_count_dframe)\n",
    "print(\"Probability of spam = \" + str(p_y_1) + \" and probability of not spam = \" + str(p_y_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "rising-carroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-spam: 59\n",
      "Spam: 170\n"
     ]
    }
   ],
   "source": [
    "#Number of times \"free\" appears in non-spam and spam respectively\n",
    "print(\"Non-spam: \" + str(len(word_count_dframe[(word_count_dframe[\"Label\"] == 0) & (word_count_dframe[\"free\"] != 0)].index)))\n",
    "print(\"Spam: \" + str(len(word_count_dframe[(word_count_dframe[\"Label\"] == 1) & (word_count_dframe[\"free\"] != 0)].index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "active-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the class-conditional probabilities, given the dataframe containing count of \n",
    "# each word in a message along with label\n",
    "def get_conditional_probs(word_count_data):\n",
    "    \n",
    "    #p(xi | y=1) => p(xi,y=1) / p(y=1) => #(xi,y=1) / #(y=1)\n",
    "    #p(xi | y=0) => p(xi,y=0) / p(y=0) => #(xi,y=0) / #(y=0)\n",
    "    count_y_1 = np.count_nonzero(word_count_data[\"Label\"] == 1)\n",
    "    count_y_0 = np.count_nonzero(word_count_data[\"Label\"] == 0)\n",
    "    p_y_1_xi = dict()\n",
    "    p_y_0_xi = dict()\n",
    "    for word in uniq_words:\n",
    "        p_y_1_xi[word] = np.count_nonzero((word_count_data[\"Label\"] == 1) & (word_count_data[word] != 0)) / count_y_1\n",
    "        p_y_0_xi[word] = np.count_nonzero((word_count_data[\"Label\"] == 0) & (word_count_data[word] != 0)) / count_y_0\n",
    "    return p_y_1_xi, p_y_0_xi\n",
    "\n",
    "p_y_1_xi, p_y_0_xi = get_conditional_probs(word_count_dframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "standard-grammar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(w=\"free\" | y=0) = 0.0122279792746114\n",
      "p(w=\"free\" | y=1) = 0.22757697456492637\n",
      "\n",
      "\n",
      "p(y=1|w=\"free\") + p(y=0|w=\"free\") should be one, Validating this below...\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print('p(w=\"free\" | y=0) = '+str(p_y_0_xi[\"free\"]))\n",
    "print('p(w=\"free\" | y=1) = ' + str(p_y_1_xi[\"free\"]) + \"\\n\\n\")\n",
    "\n",
    "print('p(y=1|w=\"free\") + p(y=0|w=\"free\") should be one, Validating this below...')\n",
    "print((p_y_1_xi[\"free\"] * p_y_1) / (len(word_count_dframe[word_count_dframe[\"free\"]!=0]) / len(word_count_dframe))\\\n",
    "+(p_y_0_xi[\"free\"] * p_y_0) / (len(word_count_dframe[word_count_dframe[\"free\"]!=0]) / len(word_count_dframe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-portable",
   "metadata": {},
   "source": [
    "**Q3: Once the above probabilities are determined, use Naive Bayes classification to classify each\n",
    "of the testing examples as spam or not. Ignore words from the testing set that are not\n",
    "contained in the training set. Report the accuracy, precision, recall and specificity, along\n",
    "with the confusion matrix for each fold. Also report the average accuracy, precision, recall\n",
    "and specificity over all folds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "asian-bouquet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________Without_laplace_______\n",
      "Accuracy: 0.9497757847533632\n",
      "True Positive: 104 True Negative:955\n",
      "False Positive: 11 False Negative:45\n",
      "Precision: 0.9043478260869565\n",
      "Speficity: 0.9886128364389234\n",
      "Recall: 0.697986577181208\n",
      "__________laplace__________\n",
      "Accuracy: 0.9623318385650225\n",
      "True Positive: 107 True Negative:966\n",
      "False Positive: 0 False Negative:42\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.7181208053691275\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9479820627802691\n",
      "True Positive: 103 True Negative:954\n",
      "False Positive: 12 False Negative:46\n",
      "Precision: 0.8956521739130435\n",
      "Speficity: 0.9875776397515528\n",
      "Recall: 0.6912751677852349\n",
      "__________laplace__________\n",
      "Accuracy: 0.9542600896860987\n",
      "True Positive: 98 True Negative:966\n",
      "False Positive: 0 False Negative:51\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.6577181208053692\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9542600896860987\n",
      "True Positive: 104 True Negative:960\n",
      "False Positive: 6 False Negative:45\n",
      "Precision: 0.9454545454545454\n",
      "Speficity: 0.9937888198757764\n",
      "Recall: 0.697986577181208\n",
      "__________laplace__________\n",
      "Accuracy: 0.957847533632287\n",
      "True Positive: 102 True Negative:966\n",
      "False Positive: 0 False Negative:47\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.6845637583892618\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9533632286995516\n",
      "True Positive: 107 True Negative:956\n",
      "False Positive: 10 False Negative:42\n",
      "Precision: 0.9145299145299145\n",
      "Speficity: 0.989648033126294\n",
      "Recall: 0.7181208053691275\n",
      "__________laplace__________\n",
      "Accuracy: 0.9695067264573991\n",
      "True Positive: 115 True Negative:966\n",
      "False Positive: 0 False Negative:34\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.7718120805369127\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9596412556053812\n",
      "True Positive: 112 True Negative:958\n",
      "False Positive: 8 False Negative:37\n",
      "Precision: 0.9333333333333333\n",
      "Speficity: 0.9917184265010351\n",
      "Recall: 0.7516778523489933\n",
      "__________laplace__________\n",
      "Accuracy: 0.9614349775784753\n",
      "True Positive: 106 True Negative:966\n",
      "False Positive: 0 False Negative:43\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.7114093959731543\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9434977578475336\n",
      "True Positive: 103 True Negative:949\n",
      "False Positive: 17 False Negative:46\n",
      "Precision: 0.8583333333333333\n",
      "Speficity: 0.9824016563146998\n",
      "Recall: 0.6912751677852349\n",
      "__________laplace__________\n",
      "Accuracy: 0.9632286995515695\n",
      "True Positive: 109 True Negative:965\n",
      "False Positive: 1 False Negative:40\n",
      "Precision: 0.990909090909091\n",
      "Speficity: 0.9989648033126294\n",
      "Recall: 0.7315436241610739\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9399103139013453\n",
      "True Positive: 97 True Negative:951\n",
      "False Positive: 15 False Negative:52\n",
      "Precision: 0.8660714285714286\n",
      "Speficity: 0.984472049689441\n",
      "Recall: 0.6510067114093959\n",
      "__________laplace__________\n",
      "Accuracy: 0.9623318385650225\n",
      "True Positive: 107 True Negative:966\n",
      "False Positive: 0 False Negative:42\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.7181208053691275\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9479820627802691\n",
      "True Positive: 100 True Negative:957\n",
      "False Positive: 9 False Negative:49\n",
      "Precision: 0.9174311926605505\n",
      "Speficity: 0.9906832298136646\n",
      "Recall: 0.6711409395973155\n",
      "__________laplace__________\n",
      "Accuracy: 0.9623318385650225\n",
      "True Positive: 107 True Negative:966\n",
      "False Positive: 0 False Negative:42\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.7181208053691275\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9524663677130045\n",
      "True Positive: 110 True Negative:952\n",
      "False Positive: 14 False Negative:39\n",
      "Precision: 0.8870967741935484\n",
      "Speficity: 0.9855072463768116\n",
      "Recall: 0.738255033557047\n",
      "__________laplace__________\n",
      "Accuracy: 0.9623318385650225\n",
      "True Positive: 107 True Negative:966\n",
      "False Positive: 0 False Negative:42\n",
      "Precision: 1.0\n",
      "Speficity: 1.0\n",
      "Recall: 0.7181208053691275\n",
      "\n",
      "\n",
      "\n",
      "________Without_laplace_______\n",
      "Accuracy: 0.9533632286995516\n",
      "True Positive: 109 True Negative:954\n",
      "False Positive: 12 False Negative:40\n",
      "Precision: 0.9008264462809917\n",
      "Speficity: 0.9875776397515528\n",
      "Recall: 0.7315436241610739\n",
      "__________laplace__________\n",
      "Accuracy: 0.9605381165919282\n",
      "True Positive: 106 True Negative:965\n",
      "False Positive: 1 False Negative:43\n",
      "Precision: 0.9906542056074766\n",
      "Speficity: 0.9989648033126294\n",
      "Recall: 0.7114093959731543\n",
      "\n",
      "\n",
      "\n",
      "----------------Total Results without Laplace-----------------\n",
      "True Positive:1049 True Negative:9546\n",
      "False Positive:114 False Negative:441\n",
      "Accuracy: 0.9502242152466368\n",
      "Precision: 0.9019776440240757\n",
      "Specificity: 0.9881987577639751\n",
      "Recall: 0.7040268456375839\n",
      "----------------Total Results with Laplace-----------------\n",
      "True Positive:1064 True Negative:9658\n",
      "False Positive:2 False Negative:426\n",
      "Accuracy: 0.9616143497757847\n",
      "Precision: 0.99812382739212\n",
      "Specificity: 0.9997929606625259\n",
      "Recall: 0.7140939597315437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# This function returns laplace transformed conditional probabilities, \n",
    "# given the dataframe containing counts of each word in message\n",
    "def get_laplace_conditional_probs(word_count_data):\n",
    "    \n",
    "    #p(xi | y=1) => p(xi,y=1) / p(y=1) => #(xi,y=1) / #(y=1)\n",
    "    #p(xi | y=0) => p(xi,y=0) / p(y=0) => #(xi,y=0) / #(y=0)\n",
    "    count_y_1 = np.count_nonzero(word_count_data[\"Label\"] == 1)\n",
    "    count_y_0 = np.count_nonzero(word_count_data[\"Label\"] == 0)\n",
    "    p_y_1_xi = dict()\n",
    "    p_y_0_xi = dict()\n",
    "    for word in uniq_words:\n",
    "        p_y_1_xi[word] = (np.count_nonzero((word_count_data[\"Label\"] == 1) & (word_count_data[word] != 0)) + 1) / (count_y_1 + len(uniq_words))\n",
    "        p_y_0_xi[word] = (np.count_nonzero((word_count_data[\"Label\"] == 0) & (word_count_data[word] != 0)) + 1) / (count_y_0 + len(uniq_words))\n",
    "    return p_y_1_xi, p_y_0_xi\n",
    "\n",
    "def predict(x_test, prior_1, prior_0, conditional_1, conditional_0, conditional_1_lap, conditional_0_lap):\n",
    "    debug=True\n",
    "\n",
    "    y_pred=[]\n",
    "    y_pred_laplace = []\n",
    "    for i in x_test.index:\n",
    "        \n",
    "        pred_1 = prior_1\n",
    "        pred_0 = prior_0\n",
    "        pred_1_lap = prior_1\n",
    "        pred_0_lap = prior_0\n",
    "        for word in x_test.columns:\n",
    "            if x[word][i] == 0 or (conditional_1[word] == 0 and conditional_0[word] == 0):\n",
    "                # IF not present in the current row OR it wasnt present in the training set\n",
    "                continue\n",
    "            \n",
    "            pred_1 *= conditional_1[word]\n",
    "            pred_0 *= conditional_0[word]\n",
    "\n",
    "            pred_1_lap *= conditional_1_lap[word]\n",
    "            pred_0_lap *= conditional_0_lap[word]\n",
    "\n",
    "        y_pred.append(1 if pred_1 > pred_0 else 0)\n",
    "        y_pred_laplace.append(1 if pred_1_lap > pred_0_lap else 0)\n",
    "    return y_pred, y_pred_laplace\n",
    "\n",
    "def print_results(y_pred, y_true):\n",
    "    fp = tp = fn = tn = 0\n",
    "    total = 0\n",
    "    for pred, true in zip(y_pred, y_true):\n",
    "        total+=1\n",
    "        if pred == true and true == 1:\n",
    "            tp+=1\n",
    "        if pred == true and true == 0:\n",
    "            tn+=1\n",
    "        if pred != true and true == 1:\n",
    "            fn+=1\n",
    "        if pred != true and true == 0:\n",
    "            fp+=1\n",
    "    print(\"Accuracy: \" + str((tp+tn)/len(y_true)))\n",
    "    print(\"True Positive: \" + str(tp) + \" True Negative:\" + str(tn))\n",
    "    print(\"False Positive: \" + str(fp) + \" False Negative:\" + str(fn))\n",
    "    print(\"Precision: \" + str(tp/(tp+fp)))\n",
    "    print(\"Speficity: \" + str(tn/(tn+fp)))\n",
    "    print(\"Recall: \" + str(tp/(tp+fn)))\n",
    "    return (tp, tn, fp, fn)\n",
    "    \n",
    "\n",
    "def fit_and_predict(train_data, x_test, y_test):\n",
    "    p_y_1, p_y_0 = get_class_probs(train_data)\n",
    "    p_y_1_xi, p_y_0_xi = get_conditional_probs(train_data)\n",
    "    p_y_1_xi_lap, p_y_0_xi_lap = get_laplace_conditional_probs(train_data)\n",
    "            \n",
    "    y_pred, y_pred_lap = predict(x_test, p_y_1, p_y_0, p_y_1_xi, p_y_0_xi, p_y_1_xi_lap, p_y_0_xi_lap)\n",
    "    print(\"________Without_laplace_______\")\n",
    "    no_lap_res = print_results(y_pred, y_test[\"Label\"])\n",
    "    print(\"__________laplace__________\")\n",
    "    lap_res = print_results(y_pred_lap, y_test[\"Label\"])\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    return no_lap_res, lap_res\n",
    "    \n",
    "x = word_count_dframe.loc[:, word_count_dframe.columns != 'Label']\n",
    "y = word_count_dframe[[\"Label\"]]\n",
    "\n",
    "folds = StratifiedShuffleSplit(n_splits=10, random_state=42, test_size=0.2)\n",
    "\n",
    "total_no_lap = (0,0,0,0)\n",
    "total_lap = (0,0,0,0)\n",
    "for train_index, test_index in folds.split(x,y):\n",
    "    train_data = word_count_dframe.iloc[train_index]\n",
    "    x_test_data = x.iloc[test_index]\n",
    "    y_test_data = y.iloc[test_index]\n",
    "    no_lap_res, lap_res = fit_and_predict(train_data, x_test_data, y_test_data)\n",
    "    total_no_lap = [i+j for i, j in zip(no_lap_res, total_no_lap)]\n",
    "    total_lap = [i+j for i, j in zip(lap_res, total_lap)]\n",
    "    \n",
    "print(\"----------------Total Results without Laplace-----------------\")\n",
    "print(\"True Positive:\" + str(total_no_lap[0]) + \" True Negative:\" + str(total_no_lap[1]))\n",
    "print(\"False Positive:\" + str(total_no_lap[2]) + \" False Negative:\" + str(total_no_lap[3]))\n",
    "print(\"Accuracy: \" + str((total_no_lap[0] + total_no_lap[1]) / sum(total_no_lap)))\n",
    "print(\"Precision: \" + str(total_no_lap[0] / (total_no_lap[0] + total_no_lap[2])))\n",
    "print(\"Specificity: \" + str(total_no_lap[1] / (total_no_lap[1] + total_no_lap[2])))\n",
    "print(\"Recall: \" + str(total_no_lap[0] / (total_no_lap[0] + total_no_lap[3])))\n",
    "\n",
    "print(\"----------------Total Results with Laplace-----------------\")\n",
    "print(\"True Positive:\" + str(total_lap[0]) + \" True Negative:\" + str(total_lap[1]))\n",
    "print(\"False Positive:\" + str(total_lap[2]) + \" False Negative:\" + str(total_lap[3]))\n",
    "print(\"Accuracy: \" + str((total_lap[0] + total_lap[1]) / sum(total_lap)))\n",
    "print(\"Precision: \" + str(total_lap[0] / (total_lap[0] + total_lap[2])))\n",
    "print(\"Specificity: \" + str(total_lap[1] / (total_lap[1] + total_lap[2])))\n",
    "print(\"Recall: \" + str(total_lap[0] / (total_lap[0] + total_lap[3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-flush",
   "metadata": {},
   "source": [
    "We have generated 10 folds with 20% test data in each fold. Above cell contains the comparative stats for each fold, when using laplace smoothing and when not using laplace smoothing. In general we see that using laplace improves the perormance across all the metrices. The precision and speficity are almost 100% for all the folds.\n",
    "\n",
    "We observe that by using laplace, following improvements in the overall metrics can be seen:\n",
    "\n",
    "Accuracy: 0.9502242152466368 -> 0.9616143497757847\n",
    "\n",
    "Precision: 0.9019776440240757 -> 0.99812382739212\n",
    "\n",
    "Specificity: 0.9881987577639751 -> 0.9997929606625259\n",
    "\n",
    "Recall: 0.7040268456375839 -> 0.7140939597315437"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-millennium",
   "metadata": {},
   "source": [
    "**Q4: Write a paragraph the summarizes the results and your thoughts about Naive Bayes classi\f",
    "-\n",
    "cation for this problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-owner",
   "metadata": {},
   "source": [
    "We see that since there are around 86.6% of non-spam messages and remaining spam message. If we predict all the messages as non-spam, we will still achieve the accuracy of 86.6%. So the accuracy is not the right measure here. We want to find out the most of the spam messages.\n",
    "\n",
    "We want all of the message which are classified as spam, to be spam. Because if an important message is classified as spam, in that case user will miss the email. User seeing a spam message, because our classifier could not classify as spam, does not impact as much as missing email does.\n",
    "\n",
    "So our metric for classifier should be (correctly predicted spam) / (all predicted spam) = precision.\n",
    "\n",
    "We see that bayesian classifier without laplace has very high accuracy, which is ~95% for all the folds. Even the precision is very high roughly ~90%. On examining the cases where the spam message classification wrongly predicted \"Spam\" or \"Not spam\", we see that in most cases it is because of presence of some specific words(gramatical errors) that are only present in spam or non-spam messages and not in both; examples include have -> hv, digits etc. In test data when we encounter such words, the probability immediately reduces to zero irrespective of what other set of words are pointing towards. With laplace smoothing this problems is resolved by not reducing the probability immediately to zero.\n",
    "\n",
    "We see that laplace smoothing takes precision to even higher level(almost 100%) than traditional bayesian classifier.\n",
    "\n",
    "Even though baysian assumption that words are not correlated is not true always. However making this general assumption, yields impressive results in our spam filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-ideal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
